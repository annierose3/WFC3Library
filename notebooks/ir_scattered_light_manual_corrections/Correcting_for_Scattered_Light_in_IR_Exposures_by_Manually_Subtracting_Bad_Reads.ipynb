{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"top\"></a>\n",
    "# Correcting for Scattered Light in IR Exposures: Manually Subtracting Bad Reads\n",
    "\n",
    "\n",
    "## Learning Goals\n",
    "\n",
    "This notebook presents one of two available methods to correct for a time variable background (TVB) due to scattered light from observing close to the Earth's limb. This method illustrates how to manually subtract any bad reads from the final exposure read of the WFC3/IR IMA data. \n",
    "\n",
    "By the end of this tutorial, you will:\n",
    "\n",
    "- Compute and plot the difference between IMA reads to identify those affected by TVB. \n",
    "- Correct a single exposure in which the first few reads are affected by scattered light by subtracting those \"bad\" reads from the final IMA read.\n",
    "- Compare the original FLT to the reprocessed FLT image.\n",
    "\n",
    "A second method for correcting TVB (masking bad reads in the RAW image and re-running `calwf3`) can be found in the notebook Correcting for Scattered Light in WFC3/IR Exposures: Using `calwf3` to Mask Bad Reads (O'Connor, in prep.). This method performs the 'ramp \n",
    "fitting' step in calwf3 and therefore removes cosmic rays from the final image. It only works, \n",
    "however, when TVB occurs at the beginning or end of the IR exposure and may leave sky \n",
    "residuals in regions flagged as IR 'blobs'.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "**Please note that the FLT products in this notebook are really 'corrected IMA' \n",
    "files and therefore do not include the 'ramp fitting' step in calwf3.  The final images will \n",
    "therefore still contain cosmic rays, and these artifacts may be removed using software such \n",
    "as AstroDrizzle when combining multiple exposures.**\n",
    "    \n",
    "</div>\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "[Introduction](#intro) <br>\n",
    "[1. Imports](#import) <br>\n",
    "[2. Downloading Data](#load) <br>\n",
    "[3. Identifying Reads with Time Variable Background](#identify) <br>\n",
    "[4. Querying CRDS for the Reference File](#ref) <br>\n",
    "\n",
    "[5. Correcting the Data](#main) <br>\n",
    "- [5.1 Excising \"Bad\" Reads](#reprocess) <br>\n",
    "- [5.2 Comparing FLT Products](#compare) <br>\n",
    "\n",
    "[6. Conclusions](#conclusions) <br>\n",
    "[Additional Resources](#add) <br>\n",
    "[About this Notebook](#about) <br>\n",
    "[Citations](#cite) <br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction <a id=\"intro\"></a>\n",
    "\n",
    "WFC3 images can be affected by many different kinds of unwanted, non-science features,  such as satellite trails, peculiar cosmic rays, Earth-limb scattered light,  reflections off the filter wheel, and more. We call these features \"anomalies\". The majority of these anomalies are well studied, correctable, and easily identifiable, as discussed in the [WFC3 2017-22 ISR](https://www.stsci.edu/files/live/sites/www/files/home/hst/instrumentation/wfc3/documentation/instrument-science-reports-isrs/_documents/2017/WFC3-2017-22.pdf).\n",
    "\n",
    "\n",
    "In this notebook, we will walk through the process of correcting WFC3/IR images for anomalies that appear in only some of the reads (see [section 7.7 of the WFC3 Instrument Handboook](https://hst-docs.stsci.edu/wfc3ihb/chapter-7-ir-imaging-with-wfc3/7-7-ir-exposure-and-readout) for a discussion of the WFC3/IR MULTIACCUM observing mode).  We examine \n",
    "an observation affected by a strong TVB due to Earth limb scattered light affecting the first few reads([section 7.10 of the WFC3 Data Handbook](https://hst-docs.stsci.edu/wfc3dhb/chapter-7-wfc3-ir-sources-of-error/7-10-time-variable-background)). We will manually subtract the affected reads from the final read of the IMA and copy the updated science and error arrays into the FLT science and error arrays.  This method is based on section 5 of  [WFC3 ISR 2016-16: 'Excising Individual Reads'](https://www.stsci.edu/files/live/sites/www/files/home/hst/instrumentation/wfc3/documentation/instrument-science-reports-isrs/_documents/2016/WFC3-2016-16.pdf).  This same method can be used to remove individual reads that contain other anomalies, such as satellite trails.\n",
    "\n",
    "This new FLT will have a reduced total exposure time (and signal-to-noise) given the rejection \n",
    "of some number of reads but will now have a flat background and an improved ramp fit.  \n",
    "\n",
    "Please see the notebook IR IMA Visualization with An Example of Time Variable Background (O'Connor, in prep.) for a walkthrough of how to identify a TVB due to scattered light. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports <a id=\"import\"></a>\n",
    "\n",
    "This notebook assumes you have created the virtual environment in [WFC3 Library's](https://github.com/spacetelescope/WFC3Library) installation instructions.\n",
    "\n",
    "We import:\n",
    "\n",
    "- *os* for setting environment variables\n",
    "- *glob* for finding lists of files\n",
    "- *shutil* for managing directories\n",
    "- *numpy* for handling array functions\n",
    "- *matplotlib.pyplot* for plotting data\n",
    "- *astropy.io fits* for accessing FITS files\n",
    "- *astroquery.mast Observations* for downloading data from MAST\n",
    "- *wfc3tools* `calwf3` for calibrating WFC3 data\n",
    "- *stsci.tools teal* for running `calwf3`\n",
    "- *ginga* for finding min/max outlier pixels\n",
    "\n",
    "We import the following modules:\n",
    "- *ima_visualization_and_differencing* to take the difference between reads, plot the ramp, and visualize the difference in images\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import glob\n",
    "import shutil \n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from astropy.io import fits\n",
    "from astroquery.mast import Mast\n",
    "from astroquery.mast import Observations\n",
    "from wfc3tools import calwf3\n",
    "from stsci.tools import teal\n",
    "from ginga.util.zscale import zscale\n",
    "\n",
    "import ima_visualization_and_differencing as diff\n",
    "\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Downloading Data<a id=\"load\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following commands query MAST for the necessary data products and downloads them to the current directory. Here we obtain WFC3/IR observations from HST Frontier Fields program [14037](https://archive.stsci.edu/proposal_search.php?id=14037&mission=hst), Visit BB. We specifically want the observation \"icqtbbbxq\", as it is strongly affected by Earth limb scattered light. The data products requested are the RAW, IMA, and FLT files.\n",
    "\n",
    "**Warning: this cell may take a few minutes to complete.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OBS_ID = 'ICQTBB020'\n",
    "data_list = Observations.query_criteria(obs_id=OBS_ID)\n",
    "\n",
    "file_id = \"icqtbbbxq\"\n",
    "Observations.download_products(data_list['obsid'], project = 'CALWF3', obs_id = file_id, \n",
    "                mrp_only = False, productSubGroupDescription = ['RAW','IMA','FLT'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will copy our RAW file into our working directory to use in this tutorial. We copy the IMA and FLT files to a new directory called \"orig/\" for later use. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('orig/'):\n",
    "    os.mkdir('orig/')\n",
    "\n",
    "shutil.copy(f'mastDownload/HST/{file_id}/{file_id}_ima.fits',f'orig/{file_id}_ima.fits')\n",
    "shutil.copy(f'mastDownload/HST/{file_id}/{file_id}_flt.fits',f'orig/{file_id}_flt.fits')  \n",
    "\n",
    "raw_file = f'mastDownload/HST/{file_id}/{file_id}_raw.fits'\n",
    "\n",
    "shutil.copy(raw_file, f'{file_id}_raw.fits')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Identifying Reads with Earth-limb Scattered Light<a id=\"identify\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we show how to identify the reads impacted by the scattered light by examining the difference in count rate between reads. This section was taken from the [IR IMA Visualization with An Example of Time Variable Background ADD LINK]() notebook, which includes a more comprehensive walkthrough of identifying time variable background. \n",
    "\n",
    "Here we implement a technique to examine the count rate difference between consecutive reads. In this case, we first convert from count rate (electrons/second) back to counts (electrons) before taking the difference, as shown in equation 3 from [WFC3 ISR 2018-05](https://www.stsci.edu/files/live/sites/www/files/home/hst/instrumentation/wfc3/documentation/instrument-science-reports-isrs/_documents/2018/WFC3-2018-05.pdf).\n",
    "\n",
    "![Instantaneous Difference Equation](instantaneous_diff.png)\n",
    "\n",
    "We compare sky values in different regions of the detector (left side, right side, and full frame). If you would like to specify your own regions for the left and right sides of your image, you can change the \"lhs_region\" and \"rhs_region\" parameters. Each region must be specified as a dictionary including the four \"corners\" (x0, x1, y0, and y1) of the region you would like to select. You may want to avoid the edges of the detector which have a large number of bad pixels and higher flat field errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (20, 10))\n",
    "\n",
    "ima = f'orig/{file_id}_ima.fits'\n",
    "\n",
    "path, filename = os.path.split(ima)\n",
    "cube, integ_time = diff.read_wfc3(ima)\n",
    "\n",
    "\n",
    "lhs_region = {\"x0\":50,\"x1\":250,\"y0\":100,\"y1\":900}\n",
    "rhs_region = {\"x0\":700,\"x1\":900,\"y0\":100,\"y1\":900}\n",
    "\n",
    "#Please use a limit that makes sense for your own data, when running your images through this notebook.\n",
    "cube[np.abs(cube) > 3] = np.nan\n",
    "\n",
    "diff_cube = diff.compute_diff_imas(cube, integ_time, diff_method = \"instantaneous\")\n",
    "median_diff_fullframe, median_diff_lhs, median_diff_rhs = diff.get_median_fullframe_lhs_rhs(diff_cube, lhs_region = lhs_region, rhs_region = rhs_region)\n",
    "\n",
    "plt.rc('xtick', labelsize = 20) \n",
    "plt.rc('ytick', labelsize = 20) \n",
    "plt.rcParams.update({'font.size': 30})\n",
    "plt.rcParams.update({'lines.markersize': 15})\n",
    "\n",
    "diff.plot_ramp(ima, integ_time, median_diff_fullframe, median_diff_lhs, median_diff_rhs)\n",
    "\n",
    "plt.ylim(0.5,2.5)\n",
    "_ = plt.title(filename)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we use the *read_wfc* funtion found in *ima_visualization_and_differencing.py* to grab the IMA data from all reads as a data cube and the integration time array, and implementing upper and lower limits on our pixel values to exclude sources when plotting our ramp. We take the instantaneous difference using the *compute_diff_imas* function from *ima_visualization_and_differencing.py*, which computes the difference as described in the equation above. Finally, We use the *plot_ramp* function from the module to plot the median count rate from the left side, right side, and full frame image.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our scattered light exposure, we see  zodiacal light at a level of ~0.9e/s in later reads, with the scattered light component affecting the first several reads where the mean count rate for the left side (orange triangles) is larger than the right side (green triangles). We can visualize this in 2 dimensions in the panel plot below, using the *panel_plot* function from the *PanelShow.py* module.  \n",
    "\n",
    "In the panel plot, we see that sources (small galaxies) are visible in the difference images using this new method. Note that this may complicate the analysis of the spatial background (e.g. left versus right) for images with extended targets, such as large galaxies. In this case, users may wish to adjust the regions of the detector used for the ramp plots. We therefore recommend inspecting both the panel plots as well as the ramp fits for diagnosing any issues with the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path,filename = os.path.split(ima)\n",
    "\n",
    "cube, integ_time = diff.read_wfc3(ima)\n",
    "\n",
    "lhs_region = {\"x0\":50,\"x1\":250,\"y0\":100,\"y1\":900}\n",
    "rhs_region = {\"x0\":700,\"x1\":900,\"y0\":100,\"y1\":900}\n",
    "\n",
    "median_fullframe, median_lhs, median_rhs = diff.get_median_fullframe_lhs_rhs(cube, lhs_region = lhs_region, rhs_region = rhs_region)\n",
    "\n",
    "diff_cube = diff.compute_diff_imas(cube, integ_time, diff_method = 'instantaneous')\n",
    "\n",
    "median_diff_fullframe, median_diff_lhs, median_diff_rhs = diff.get_median_fullframe_lhs_rhs(diff_cube, lhs_region = lhs_region, rhs_region = rhs_region)\n",
    "standard_dev_fullframe, standard_dev_lhs, standard_dev_rhs = diff.get_median_std_fullframe_lhs_rhs(diff_cube, lhs_region = lhs_region, rhs_region = rhs_region)\n",
    "\n",
    "fig_0 = diff.panel_plot(cube, integ_time, median_diff_fullframe, median_diff_lhs, median_diff_rhs, standard_dev_fullframe, standard_dev_lhs, standard_dev_rhs, diff_method = 'instantaneous')\n",
    "_ = fig_0.suptitle(filename, fontsize = 40)\n",
    "plt.subplots_adjust(bottom = 0.25, right = 0.9, top = 0.95)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this figure, we see that the ratio of instantaneous rate for the left versus right side of the image is ~1.0 for all but the first few reads (which are affected by scattered light). We choose to exclude reads with a ratio greater than 1.1 e-/s from the final science data, as they are the reads most strongly affected by scattered light. While this reduces the total exposure from 1403 to 1000 seconds, it removes the spatial component from the sky background."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Querying CRDS for the Reference File<a id=\"ref\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will run `calwf3` to produce an IMA file, which we will use to remove bad reads. Before running `calwf3`, we need to set some environment variables.\n",
    "\n",
    "We will point to a subdirectory called crds_cache/ using the IREF environment variable, which is used for WFC3 reference files. Other instruments use other variables, e.g., JREF for ACS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CRDS_SERVER_URL'] = 'https://hst-crds.stsci.edu'\n",
    "os.environ['CRDS_SERVER'] = 'https://hst-crds.stsci.edu'\n",
    "os.environ['CRDS_PATH'] = './crds_cache'\n",
    "os.environ['iref'] = './crds_cache/references/hst/wfc3/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code block below will query CRDS for the best reference files currently available for our dataset and update the header keywords to point to these new files. We will use the Python package *os* to run terminal commands. In the terminal, the line would be:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "crds bestrefs --files [filename] --sync-references=1 --update-bestrefs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...where 'filename' is the name of your fits file.\n",
    "\n",
    "\n",
    "**Warning: this cell may take a few minutes to complete.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_file = f'{file_id}_raw.fits'\n",
    "\n",
    "\n",
    "print( f\"Querying CRDS for the reference file associated with {raw_file}.\")\n",
    "command_line_input = 'crds bestrefs --files {:} --sync-references=1 --update-bestrefs'.format(raw_file)\n",
    "os.system(command_line_input);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Correcting the Data<a id=\"main\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To address anomalies such as scattered light, satellite trails, and more in WFC3/IR images  we can remove the individual affected reads and correct the science and error arrays accordingly. For our example image, we choose to exclude reads where the ratio of background signal is greater than 1.1 e-/s (see the notebook [IR IMA Visualization with An Example of Time Variable Background]() for a more complete demonstration of how we find this ratio).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Excising \"Bad\" Reads <a id=\"reprocess\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we walk through the key steps to removing bad reads. \n",
    "\n",
    "We select our excluded reads (in this case reads reads SCI,15 through SCI, 11) and use the \"remove_reads\" function to effectively subtract out the reads affected by TVB. \n",
    "\n",
    "The key steps to this process are:\n",
    "\n",
    "1. Run `calwf3` on the RAW image file to produce an IMA and an FLT.\n",
    "2. Take the IMA file and subtract the science array from the \"bad\" reads from the cumulative exposure.\n",
    "    - Correct the final integration time accordingly. \n",
    "3. Correct the error image extension to reflect the changes to the science array. \n",
    "4. Update the final FLT headers, science array, and error array.\n",
    "\n",
    "\n",
    "We walk through this process below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### STEP 1: Run `calwf3`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_filepath = f'{file_id}_raw.fits'\n",
    "bad_reads = [11,12,13,14,15] #note, we leave the \"zero read\" (read number 16)\n",
    "\n",
    "#### Remove any existing ima or flt products from working directory or calwf3 will not run \n",
    "for ext in ['flt','ima']:\n",
    "    if os.path.exists(raw_filepath.replace('raw', ext)):\n",
    "        os.remove(raw_filepath.replace('raw', ext))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " ##run CALWF3\n",
    "calwf3(raw_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### STEP 2: Correct the IMA product"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we walk through the process of subtracting the reads affected by Earth-limb scattered light from the cumulative science extension of the IMA. While an FLT product is also created in step 1, we do not use this file due to the poor-quality ramp fit in the presence of TVB.\n",
    "\n",
    "Overall, the steps are as follows:\n",
    "\n",
    "1. Grab the science data as a 3D array and the integration time as a 1D array from the science and time extensions of the IMA file and the associated dark reference file. \n",
    "2. Convert science data from electrons/second to electrons.\n",
    "3. Compute the difference between reads (the cumulative difference, in this case) to isolate counts and integration time from bad reads.\n",
    "4. For each bad read, subtract the counts (and time) from ONLY that read from the total data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ima = fits.open(raw_filepath.replace('raw', 'ima'))\n",
    "\n",
    "#grab the science data as a 3D array and the integration time as a 1D array\n",
    "cube, integ_time = diff.read_wfc3(raw_filepath.replace('raw','ima'))\n",
    "\n",
    "\n",
    "#do the same for the corresponding dark image. The dark reference file used for the observation can be found in the primary header\n",
    "dark_file = ima[0].header['DARKFILE'].replace('iref$', os.getenv('iref')+'/')\n",
    "dark_cube, dark_time = diff.read_wfc3(dark_file)\n",
    "\n",
    "\n",
    "#convert from count rate to counts\n",
    "cube_counts = np.zeros(np.shape(cube))\n",
    "dark_counts = np.zeros(np.shape(dark_cube))\n",
    "for img in range(cube.shape[2]):\n",
    "    cube_counts[:,:,img] = cube[:,:,img]*integ_time[img]\n",
    "    dark_counts[:,:,img ] = dark_cube[:,:,img]/dark_time[img]*integ_time[img]\n",
    "    \n",
    "    \n",
    "#Compute the difference between reads (the cumulative difference, in this case) to isolate counts (and integration time) from bad reads\n",
    "difference = np.diff(cube_counts, axis = 2)\n",
    "dark_diff = np.diff(dark_counts, axis = 2)\n",
    "dt = np.diff(integ_time)\n",
    "\n",
    "#For each bad read, subtract the counts (or time) from ONLY that read from the total data\n",
    "final_dark = dark_counts[:,:,-1]*1  \n",
    "final_sci = cube_counts[:,:,-1]*1\n",
    "final_time = np.ones((1024, 1024))*integ_time[-1]\n",
    "if (len(bad_reads) > 0):\n",
    "    for read in bad_reads:\n",
    "        index = cube_counts.shape[2]-read-1 #because the reads are stored in reverse order\n",
    "        final_sci -= difference[:,:,index]\n",
    "        final_dark -= dark_diff[:,:,index]\n",
    "        final_time -= dt[index]\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### STEP 3: Correct the Error Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The errors associated with the raw data are estimated according  to the noise model for the detector which currently includes a simple combination of detector readnoise and poisson noise from the pixel and which is intended to replicate the calwf3 error array calculation. Currently, the inital detector noise model (in electrons) is as follows:\n",
    "\n",
    "$$ SCI_{1} = signal= (flux*flat + dark)$$ \n",
    "\n",
    "$$ \\sigma_{1} = \\sqrt{(RN^{2}) + (flux*flat + dark)} $$\n",
    "\n",
    "\n",
    "Note that our signal equation includes flat and dark corrections to revert the final science array (flux) produced by `calwf3` to the original signal (in electrons) recorded in the detector.\n",
    "\n",
    "The ERR array continues to be updated as the SCI array is processed. The dark error term is added in quadrature as the dark current is subtracted from the science array:\n",
    "\n",
    "$$ SCI_{2} = signal - dark $$\n",
    "\n",
    "$$ \\sigma_{2} = \\sqrt{\\sigma_{1}^{2}+\\sigma_{dark}^{2}} $$\n",
    "\n",
    "Next, the flat images are divided out of the science image. There are two flat-fields (p-flat and delta-flat), which are each divided out of the science image. The flat errors are combined using the correct error propagation method.\n",
    "\n",
    " $$ flat = pflat * dflat $$\n",
    "\n",
    "P-Flat (pflat) file:\n",
    "\n",
    "$$ SCI_{3} = SCI_{2}/pflat $$\n",
    "\n",
    "$$ \\frac{\\sigma_{3}}{SCI_{3}} = \\sqrt{ (\\frac{\\sigma_{2}}{SCI_{2}})^{2} +(\\frac{\\sigma_{pflat}}{pflat})^{2}} $$\n",
    "\n",
    "Delta-Flat (dflat) file:\n",
    "\n",
    "$$ SCI_{4} = SCI_{3}/dflat $$\n",
    "\n",
    "$$ \\frac{\\sigma_{4}}{SCI_{4}} = \\sqrt{ (\\frac{\\sigma_{3}}{SCI_{3}})^{2} +(\\frac{\\sigma_{dflat}}{dflat})^{2}} = \\sqrt{ (\\frac{\\sigma_{2}}{SCI_{2}})^{2} +(\\frac{\\sigma_{pflat}}{pflat})^{2} +(\\frac{\\sigma_{dflat}}{dflat})^{2}}$$\n",
    "\n",
    "\n",
    "and finally it is converted to an error in electrons/second as:\n",
    "\n",
    " $$ \\sigma_{e^{-}/s}= \\frac{\\sigma_{4}}{exptime}$$\n",
    " \n",
    "Evaluating this equation, we come up with an equation for error array calculation (in electrons/second) which we use in our python calculations as follows:\n",
    "\n",
    "$$ \\sigma_{e^{-}/s}= \\frac{\\sqrt{ RN^{2} + (flux*(pflat*dflat) + dark) +  \\sigma_{dark}^{2} + (\\sigma_{pflat}*flux*dflat)^{2} + (\\sigma_{dflat}*flux*pflat)^{2} }}{pflat*dflat*exptime}$$\n",
    "\n",
    "If we treat the pflat and dflat terms as one general flat term, we can rewrite the error equation more simply as:\n",
    "\n",
    "$$ \\sigma_{e^{-}/s}= \\frac{\\sqrt{ RN^{2} + (flux*flat + dark) +  \\sigma_{dark}^{2} + (\\sigma_{flat}*flux*flat)^{2} }}{flat*exptime}$$\n",
    "\n",
    "\n",
    "We do not address the error propagation through the nonlinearity correction step of `calwf3` (NLINCORR), as it is complicated and sufficiently small to be outside of the scope of this notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NSAMP = ima[0].header['NSAMP']\n",
    "##getting variance terms\n",
    "#### Readnoise in 4 amps\n",
    "readnoise_2D = np.zeros((1024,1024))\n",
    "readnoise_2D[512: ,0:512] += ima[0].header['READNSEA']\n",
    "readnoise_2D[0:512,0:512] += ima[0].header['READNSEB']\n",
    "readnoise_2D[0:512, 512:] += ima[0].header['READNSEC']\n",
    "readnoise_2D[512: , 512:] += ima[0].header['READNSED']\n",
    "\n",
    "\n",
    "#### Gain in 4 amps\n",
    "gain_2D = np.zeros((1024,1024))\n",
    "gain_2D[512: ,0:512] += ima[0].header['ATODGNA']\n",
    "gain_2D[0:512,0:512] += ima[0].header['ATODGNB']\n",
    "gain_2D[0:512, 512:] += ima[0].header['ATODGNC']\n",
    "gain_2D[512: , 512:] += ima[0].header['ATODGND']\n",
    "\n",
    "#dark image\n",
    "dark_im = fits.open(dark_file)\n",
    "\n",
    "##And grabbing the flats\n",
    "pflat_file = ima[0].header['PFLTFILE'].replace('iref$', os.getenv('iref')+'/')\n",
    "pflat_im = fits.open(pflat_file)\n",
    "pflat = pflat_im[1].data\n",
    "pflat_error=pflat_im['ERR'].data\n",
    "\n",
    "dflat_file = ima[0].header['DFLTFILE'].replace('iref$', os.getenv('iref')+'/')\n",
    "dflat_im = fits.open(dflat_file)\n",
    "dflat = dflat_im[1].data\n",
    "dflat_error=dflat_im['ERR'].data\n",
    "\n",
    "#Computing the final error\n",
    "\n",
    "#### Variance terms\n",
    "### read noise\n",
    "RN = readnoise_2D\n",
    "## poisson error: sqrt(signal), flux = final_sci, dark is converted to electrons\n",
    "signal = (final_sci*(pflat*dflat)+final_dark*gain_2D)\n",
    "## flat errors\n",
    "#pflat\n",
    "pflat_err = (pflat_im['ERR'].data)\n",
    "#dflat\n",
    "dflat_err = (dflat_im['ERR'].data)\n",
    "#dark error\n",
    "dark_err = (dark_im['ERR'].data)*gain_2D \n",
    "##final\n",
    "final_err = np.sqrt(RN**2 + signal + (dark_err**2) + (pflat_err*final_sci*dflat)**2\\\n",
    "                  + (dflat_err*final_sci*pflat)**2)\n",
    "    \n",
    "final_err /= (pflat*dflat*final_time)\n",
    "\n",
    "#and finally the final science image (converted back to count rate)\n",
    "final_sci /= final_time      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### STEP 4: Update the FLT Product"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we update the FLT product created in step 1 with the new science array, error array, and header keywords (such as the new total exposure time)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flt = fits.open(raw_filepath.replace('raw', 'flt'), mode='update')\n",
    " \n",
    "#Updating the flt data\n",
    "\n",
    "flt['SCI'].data = final_sci[5:-5,5:-5]\n",
    "flt['ERR'].data = final_err[5:-5,5:-5]\n",
    "\n",
    "\n",
    "# Updating the FLT header\n",
    "flt[0].header['IMA2FLT'] = (1, 'FLT extracted from IMA file')\n",
    "flt[0].header['EXPTIME'] = np.max(final_time)\n",
    "flt[0].header['NPOP'] = (len(bad_reads), 'Number of reads popped from the sequence')\n",
    "\n",
    "flt.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Putting It All Together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we have included the entire correction process (steps 1-4 above) in a function, for convenience. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def remove_reads(raw_filepath, bad_reads = []):\n",
    "    '''\n",
    "    From the final IMA read, subtract data of reads affected by anomalies.\n",
    "    Compute a corrected science image and error image, and place them in the FLT product. \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    raw_filepath: str\n",
    "        Path to a RAW full-frame IR image fits file.\n",
    "        \n",
    "    bad_reads: list of int\n",
    "        List of the IMSET numbers (science extension numbers) of reads affected by anomalies.\n",
    "    '''\n",
    "    \n",
    "    #### Remove existing products or calwf3 will die\n",
    "    for ext in ['flt','ima']:\n",
    "        if os.path.exists(raw_filepath.replace('raw', ext)):\n",
    "            os.remove(raw_filepath.replace('raw', ext))\n",
    "    \n",
    "    #### Run calwf3\n",
    "    calwf3(raw_filepath)\n",
    "    \n",
    "    #take calwf3 produced IMA, get rid of bad reads \n",
    "    flt = fits.open(raw_filepath.replace('raw', 'flt'), mode = 'update')\n",
    "    ima = fits.open(raw_filepath.replace('raw', 'ima'))\n",
    "    \n",
    "    NSAMP = ima[0].header['NSAMP']\n",
    "    ##getting variance terms\n",
    "    #### Readnoise in 4 amps\n",
    "    readnoise_2D = np.zeros((1024,1024))\n",
    "    readnoise_2D[512: ,0:512] += ima[0].header['READNSEA']\n",
    "    readnoise_2D[0:512,0:512] += ima[0].header['READNSEB']\n",
    "    readnoise_2D[0:512, 512:] += ima[0].header['READNSEC']\n",
    "    readnoise_2D[512: , 512:] += ima[0].header['READNSED']\n",
    "    \n",
    "\n",
    "    #### Gain in 4 amps\n",
    "    gain_2D = np.zeros((1024,1024))\n",
    "    gain_2D[512: ,0:512] += ima[0].header['ATODGNA']\n",
    "    gain_2D[0:512,0:512] += ima[0].header['ATODGNB']\n",
    "    gain_2D[0:512, 512:] += ima[0].header['ATODGNC']\n",
    "    gain_2D[512: , 512:] += ima[0].header['ATODGND']\n",
    "    \n",
    "    #getting darks\n",
    "    dark_file = ima[0].header['DARKFILE'].replace('iref$', os.getenv('iref')+'/')\n",
    "    dark_im = fits.open(dark_file)\n",
    "    \n",
    "    #and flats\n",
    "    pflat_file = ima[0].header['PFLTFILE'].replace('iref$', os.getenv('iref')+'/')\n",
    "    pflat_im = fits.open(pflat_file)\n",
    "    pflat = pflat_im[1].data\n",
    "    pflat_error = pflat_im['ERR'].data\n",
    "    \n",
    "    dflat_file = ima[0].header['DFLTFILE'].replace('iref$', os.getenv('iref')+'/')\n",
    "    dflat_im = fits.open(dflat_file)\n",
    "    dflat = dflat_im[1].data\n",
    "    dflat_error = dflat_im['ERR'].data\n",
    "    \n",
    "    cube, integ_time = diff.read_wfc3(raw_filepath.replace('raw','ima'))\n",
    " \n",
    "    \n",
    "    dark_cube, dark_time = diff.read_wfc3(dark_file) \n",
    "\n",
    "    \n",
    "    #from rate to counts \n",
    "    cube_counts = np.zeros(np.shape(cube))\n",
    "    dark_counts = np.zeros(np.shape(dark_cube))\n",
    "    for img in range(cube.shape[2]):\n",
    "        cube_counts[:,:,img] = cube[:,:,img]*integ_time[img]\n",
    "        dark_counts[:,:,img] = dark_cube[:,:,img]/dark_time[img]*integ_time[img]\n",
    "    \n",
    "    #diff between reads\n",
    "    difference = np.diff(cube_counts, axis=2)\n",
    "    dark_diff = np.diff(dark_counts, axis=2)\n",
    "    dt = np.diff(integ_time)\n",
    "    \n",
    "    \n",
    "    final_dark = dark_counts[:,:,-1]*1  \n",
    "    final_sci = cube_counts[:,:,-1]*1\n",
    "    final_time = np.ones((1024, 1024))*integ_time[-1]\n",
    "    if (len(bad_reads) > 0):\n",
    "        for read in bad_reads:\n",
    "            index = cube_counts.shape[2]-read\n",
    "            final_sci -= difference[:,:,index]\n",
    "            final_dark -= dark_diff[:,:,index]\n",
    "            final_time -= dt[index]\n",
    "            \n",
    "    \n",
    "    \n",
    "   #Computing the final error\n",
    "\n",
    "    #### Variance terms\n",
    "    ### read noise\n",
    "    RN = readnoise_2D\n",
    "    ## poisson error: sqrt(signal), flux = final_sci, dark is converted to electrons\n",
    "    signal = (final_sci*(pflat*dflat)+final_dark*gain_2D)\n",
    "    ## flat errors\n",
    "    #pflat\n",
    "    pflat_err = (pflat_im['ERR'].data)\n",
    "    #dflat\n",
    "    dflat_err =(dflat_im['ERR'].data)\n",
    "    #dark error\n",
    "    dark_err = (dark_im['ERR'].data)*gain_2D \n",
    "    ##final\n",
    "    final_err = np.sqrt(RN**2 + signal + (dark_err**2) + (pflat_err*final_sci*dflat)**2\\\n",
    "                      + (dflat_err*final_sci*pflat)**2)\n",
    "\n",
    "    final_err /= (pflat*dflat*final_time)\n",
    "    \n",
    "    #and finally the final science image (converted back to count rate)\n",
    "    final_sci /= final_time      \n",
    "    \n",
    "    #updating the flt\n",
    "\n",
    "    flt['SCI'].data = final_sci[5:-5,5:-5]\n",
    "    flt['ERR'].data = final_err[5:-5,5:-5]\n",
    "    \n",
    "   \n",
    "     ### Update the FLT header\n",
    "    flt[0].header['IMA2FLT'] = (1, 'FLT extracted from IMA file')\n",
    "    flt[0].header['EXPTIME'] = np.max(final_time)\n",
    "    flt[0].header['NPOP'] = (len(bad_reads), 'Number of reads popped from the sequence')\n",
    "    \n",
    "    \n",
    "    for iread, read in enumerate(bad_reads):\n",
    "        flt[0].header['POPRD%02d' %(iread+1)] = (read, 'Read kicked out of the MULTIACCUM sequence')\n",
    "    \n",
    "    flt.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Comparing FLT Products <a id=\"compare\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may want to move your reprocessed images to a new directory, especially if you plan to run the notebook again. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reprocessed_flt = f'{file_id}_flt.fits'\n",
    "reprocessed_ima = f'{file_id}_ima.fits'\n",
    "original_flt = f'orig/{file_id}_flt.fits'\n",
    "original_ima = f'orig/{file_id}_ima.fits'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can compare our original and reprocessed FLT products."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "image_new = fits.open(reprocessed_flt)\n",
    "image_old = fits.open(original_flt)\n",
    "\n",
    "fig = plt.figure(figsize = (20, 7))\n",
    "fig\n",
    "rows = 1\n",
    "columns = 2\n",
    "\n",
    "#add the total exptime in the title \n",
    "ax1 = fig.add_subplot(rows, columns, 1)\n",
    "ax1.set_title(\"Reprocessed FLT image\", fontsize = 20)\n",
    "im1 = plt.imshow(image_new[\"SCI\", 1].data, vmin = 0.8, vmax = 1.3, origin = 'lower', cmap = 'Greys_r')\n",
    "ax1.tick_params(axis = 'both',labelsize = 10)\n",
    "cbar1 = plt.colorbar(im1, ax = ax1)\n",
    "cbar1.ax.tick_params(labelsize = 10)\n",
    "\n",
    "ax2 = fig.add_subplot(rows, columns, 2)\n",
    "ax2.set_title(\"Original FLT image\", fontsize = 20)\n",
    "im2=plt.imshow(image_old[\"SCI\", 1].data, vmin = 0.8, vmax = 1.3, origin = 'lower',  cmap = 'Greys_r')\n",
    "ax2.tick_params(axis = 'both', labelsize = 10)\n",
    "cbar2 = plt.colorbar(im2, ax = ax2)\n",
    "cbar2.ax.tick_params(labelsize = 10)\n",
    "\n",
    "plt.rc('xtick', labelsize = 10) \n",
    "plt.rc('ytick', labelsize = 10) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This new image was produced by subtracting the signal from the first 5 reads (not including the 0 read) from the final science data, reducing the effective exposure time from 1403 to 1000 seconds. While the total exposure is reduced from 1403 seconds to 1000 seconds (thus decreasing the overall S/N of the image), the background in the reprocessed image is now uniform over the entire field of view. We can see that the new FLT image is free of the Earth limb scattered light visible in the old FLT image. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The final exposure time after reprocessing is {}.'.format(image_new[0].header['EXPTIME']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Conclusions <a id=\"conclusions\"></a>\n",
    "\n",
    "**Congratulations, you have completed the notebook.**\n",
    "\n",
    "You should now be familiar with how to reprocess an observation affected by Earth limb scattered light by removing the affected reads from your science and error images. \n",
    "\n",
    "Thank you for following along! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Resources <a id=\"add\"></a>\n",
    "\n",
    "\n",
    "Below are some additional resources that may be helpful. Please send any questions through the [HST Help Desk](https://stsci.service-now.com/hst).\n",
    "\n",
    "- [WFC3 Website](https://www.stsci.edu/hst/instrumentation/wfc3)\n",
    "    - [WFC3/UVIS CTE Website](https://www.stsci.edu/hst/instrumentation/wfc3/performance/cte)\n",
    "- [WFC3 Instrument Handbook](https://hst-docs.stsci.edu/wfc3ihb)\n",
    "- [WFC3 Data Handbook](https://hst-docs.stsci.edu/wfc3dhb)\n",
    "- [Instrument Science Report WFC3 2021-01](https://www.stsci.edu/files/live/sites/www/files/home/hst/instrumentation/wfc3/documentation/instrument-science-reports-isrs/_documents/2021/2021-01.pdf)\n",
    "\n",
    "## About this Notebook <a id=\"about\"></a>\n",
    "\n",
    "\n",
    "**Author:** Anne O'Connor, Jennifer Mack, Annalisa Calamida, Harish Khandrika -- WFC3 Instrument\n",
    "\n",
    "**Updated On:** 2023-03-24\n",
    "\n",
    "## Citations <a id=\"cite\"></a>\n",
    "\n",
    "If you use the following tools for published research, please cite the\n",
    "authors. Follow these links for more information about citing the tools:\n",
    "\n",
    "* [Citing `numpy`](https://numpy.org/citing-numpy/)\n",
    "* [Citing `astropy`](https://www.astropy.org/acknowledging.html)\n",
    "* [Citing `astroquery`](https://astroquery.readthedocs.io/en/latest/)\n",
    "* [Citing `wfc3tools`](https://wfc3tools.readthedocs.io/en/latest/)\n",
    "* [Citing `drizzlepac`](https://drizzlepac.readthedocs.io/en/latest/LICENSE.html)\n",
    "\n",
    "\n",
    "If you use this notebook, or information from the WFC3 Data Handbook, Instrument Handbook,\n",
    "or WFC3 ISRs for published research, please cite them:\n",
    "\n",
    "* [Citing the WFC3 Data Handbook](https://hst-docs.stsci.edu/wfc3dhb#:~:text=Citation,%2C%20(Baltimore%3A%20STScI).)\n",
    "* [Citing the WFC3 Instrument Handbook](https://hst-docs.stsci.edu/wfc3ihb#:~:text=Citation,14.0%E2%80%9D%20(Baltimore%3A%20STScI))\n",
    "* [Instrument Science Report WFC3 2021-01](https://www.stsci.edu/files/live/sites/www/files/home/hst/instrumentation/wfc3/documentation/instrument-science-reports-isrs/_documents/2021/2021-01.pdf)\n",
    "* Citing this notebook:\n",
    "    Please cite the primary author and year, and hyperlink the notebook or WFC3 Library\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Top of Page](#top)\n",
    "<img style=\"float: right;\" src=\"https://raw.githubusercontent.com/spacetelescope/notebooks/master/assets/stsci_pri_combo_mark_horizonal_white_bkgd.png\" alt=\"Space Telescope Logo\" width=\"200px\"/> "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
